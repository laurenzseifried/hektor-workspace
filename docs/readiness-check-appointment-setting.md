# Readiness Check ‚Äî Appointment Setting Business

**Status:** IN PROGRESS  
**Ziel:** Alle internen Prozesse + Configs validieren BEVOR wir externe Services (Apollo, Hunter, Calendly) abonnieren  
**Created:** 2026-02-11 14:46 GMT+1  
**Owner:** Hektor + Scout

---

## Warum diese Pr√ºfung?

**Problem:** Geld ausgeben w√§hrend interner Prozess noch hakt = verschwendet Budget + Zeit  
**L√∂sung:** Alles intern validieren, Bugs fixen, Prozess stabilisieren ‚Üí DANN externe Services

---

## Pr√ºfpunkte (von Laurenz)

### 1. Dashboard Features f√ºr Lead Gen Business

**Frage:** Hat das Dashboard alle n√∂tigen Features um als Grundlage f√ºr das AS Business zu dienen?

**Ben√∂tigte Features (aus Implementation Plan):**

| Feature | Status | Was fehlt? | Priorit√§t |
|---------|--------|-----------|-----------|
| **AS Clients Page** | ‚ùå FEHLT | Liste aller Clients, ICP Management, Metrics | HOCH |
| **Lead Pipeline (Kanban)** | ‚ùå FEHLT | Raw ‚Üí Enriched ‚Üí Contacted ‚Üí Replied ‚Üí Booked ‚Üí Completed | HOCH |
| **Outreach Metrics** | ‚ùå FEHLT | Charts: Emails sent, Reply rate, Booking rate | MITTEL |
| **Weekly Reports** | ‚ùå FEHLT | Auto-generierte PDFs f√ºr Clients | MITTEL |
| **Activity Logging** | ‚úÖ VORHANDEN | Bereits da, aber: Log AS-spezifische Activities? | NIEDRIG |
| **Tasks** | ‚úÖ VORHANDEN | K√∂nnen f√ºr AS-Tasks genutzt werden | OK |
| **Projects** | ‚úÖ VORHANDEN | K√∂nnen f√ºr Clients genutzt werden (Workaround) | OK |

**Fehlende Features:**
1. AS Clients Page (API + UI)
2. Lead Pipeline (Kanban board)
3. Outreach Metrics (Charts)
4. Weekly Reports (PDF export)

**Kann Claude Code bauen?** JA (2-3 Tage)

**Alternative (kurzfristig):**
- Clients in Projects verwalten (Workaround)
- Leads in Tasks tracken (Workaround)
- Metrics manuell in Google Sheets (Workaround)

**Empfehlung:**
- **Option A:** Claude Code baut Features JETZT (2-3 Tage), dann starten
- **Option B:** Starten mit Workarounds (Projects + Tasks), Features parallel bauen

**Entscheidung ben√∂tigt:** Option A oder B?

---

### 2. Aufgabensplit Scout/Hektor f√ºr AS Business

**Frage:** Ist der Job Split zwischen Hektor und Scout passend f√ºr Appointment Setting?

**Aktueller Split (aus Implementation Plan):**

| Phase | Scout | Hektor | Passend? |
|-------|-------|--------|----------|
| Client Onboarding | ‚ùå | ‚úÖ | ‚úÖ JA |
| Lead Research | ‚úÖ | ‚ùå | ‚úÖ JA |
| Lead Enrichment | ‚ùå (pre-enrichment) | ‚úÖ (scoring) | ‚úÖ JA |
| Outreach Drafting | ‚ùå | ‚úÖ | ‚úÖ JA |
| Outreach Execution | ‚ùå | ‚úÖ | ‚úÖ JA |
| Meeting Booking | ‚ùå | ‚úÖ | ‚úÖ JA |
| Reporting | ‚ùå (data aggregation) | ‚úÖ (insights) | ‚úÖ JA |

**Optimierung vorgeschlagen:**
- Scout macht **Pre-Enrichment** (Basic Company Research w√§hrend Lead Finding)
- Hektor fokussiert auf **Scoring + Qualification**
- Scout generiert **Weekly Report Draft** (Hektor f√ºgt Insights hinzu)

**Status:** ‚úÖ Split ist passend

**Weitere Optimierungen n√∂tig?**
- ‚ö†Ô∏è Scout k√∂nnte **Email Response Monitoring** √ºbernehmen (Inbox Triage)
- ‚ö†Ô∏è Hektor k√∂nnte **A/B Test Analyse** automatisieren (welche Templates gewinnen?)

**Test ben√∂tigt:** 1-2 Wochen Praxis ‚Üí dann optimieren

---

### 3. Configs passend f√ºr AS Business

**Frage:** Spiegeln eure Configs das AS Business passend wieder?

**Zu pr√ºfen:**

#### 3.1 AGENTS.md

**Aktueller Stand:**
- Model Routing Framework: ‚úÖ Definiert
- Autonomie-Regeln: ‚úÖ Definiert
- Fehlerbehandlung: ‚úÖ Self-Heal Protocol
- Task-Disziplin: ‚úÖ Dashboard-Kanban
- Memory: ‚úÖ Daily Logs + Langzeit

**AS-spezifische Erg√§nzungen n√∂tig:**
- [ ] Lead Research Workflow (Scout's Prozess)
- [ ] Lead Enrichment Workflow (Hektor's Scoring)
- [ ] Outreach Workflow (Email Templates, Follow-ups)
- [ ] Meeting Booking Workflow (Calendly Integration)
- [ ] Client Onboarding Workflow (ICP Definition)

**Action:** Erg√§nze AGENTS.md mit AS-Workflows (Referenz: `/docs/appointment-setting-implementation.md`)

---

#### 3.2 HEARTBEAT.md

**Aktueller Stand (Hektor):**
```
1. Dashboard Check (dashboard briefing)
2. Decision Tree:
   - IF blockers > 0: Self-Heal
   - ELSE IF tasks in-progress: Continue
   - ELSE IF backlog high-priority: Pull task
   - ELSE: Check Scout, NO_REPLY if quiet
3. Response: Brief summary or HEARTBEAT_OK
```

**AS-spezifisch erg√§nzen:**
- [ ] Check: Neue Leads von Scout? ‚Üí Enrichment starten
- [ ] Check: Outreach Responses? ‚Üí Reply to positives, book meetings
- [ ] Check: No-shows gestern? ‚Üí Replace meetings
- [ ] Check: Weekly Report f√§llig? ‚Üí Generate + send

**Scout's HEARTBEAT.md:**
- Existiert in `/Users/laurenz/.openclaw/workspace-scout/HEARTBEAT.md`?
- Pr√ºfen: Ist es AS-spezifisch?

**Action:** Erg√§nze HEARTBEAT.md (Hektor + Scout) mit AS-spezifischen Checks

---

#### 3.3 OpenClaw Gateway Config

**Aktueller Stand:** `openclaw.json`

**AS-spezifische Configs n√∂tig:**
- [ ] SMTP Config (f√ºr Email Sending)
- [ ] Webhook Endpoints (f√ºr Calendly Integration)
- [ ] API Keys (Apollo, Hunter) in `.env`
- [ ] Message Templates (Outreach Emails) in `memory/templates/`

**Pr√ºfung:**
```bash
# Check: Ist SMTP konfiguriert?
openclaw config get messaging.smtp

# Check: Sind Webhook Endpoints enabled?
openclaw config get webhooks.enabled

# Check: Sind API Keys gesetzt?
echo $APOLLO_API_KEY
echo $HUNTER_API_KEY
```

**Action:** Config-Erg√§nzungen vornehmen (nach Laurenz OK)

---

#### 3.4 TOOLS.md

**Aktueller Stand:**
- Telegram Topic IDs: ‚úÖ Updated (lead-gen = 795)
- Model Routing: ‚úÖ Referenziert
- Dashboard API: ‚úÖ Dokumentiert

**AS-spezifisch erg√§nzen:**
- [ ] Apollo API Usage (Rate Limits, Queries)
- [ ] Hunter API Usage (Rate Limits)
- [ ] Calendly Webhook Format
- [ ] Email Templates Location

**Action:** Erg√§nze TOOLS.md mit AS-spezifischen Tool-Infos

---

### 4. HEARTBEAT.md autonom + Ollama-konform

**Frage:** Sind Heartbeats so konfiguriert, dass AS Business autonom l√§uft?

**Ollama Heartbeat Check (Hektor):**
- Model: `ollama/llama3.2:3b`
- Cadence: 30min
- Tools: `session_status`, `sessions_send` (minimal)

**Pr√ºfung:**
- ‚úÖ Hektor Heartbeat funktioniert (Ollama)
- ‚ùì Scout Heartbeat konfiguriert? (f√ºr AS)

**AS-spezifische Heartbeat-Logic:**

**Hektor (alle 30min):**
1. Check Dashboard: Neue Leads von Scout?
   - JA ‚Üí Starte Enrichment (Haiku/Sonnet je nach Complexity)
   - NEIN ‚Üí NO_REPLY
2. Check Inbox: Neue Responses?
   - JA ‚Üí Kategorisiere (Positive ‚Üí Book Meeting)
   - NEIN ‚Üí NO_REPLY
3. Check Calendar: No-shows gestern?
   - JA ‚Üí Replace Meetings
   - NEIN ‚Üí NO_REPLY
4. Check: Weekly Report f√§llig?
   - JA ‚Üí Generate Report (Sonnet)
   - NEIN ‚Üí NO_REPLY

**Scout (alle 60min):**
1. Check Dashboard: Client braucht neue Leads?
   - JA ‚Üí Query Apollo, Recherchiere, Poste Findings
   - NEIN ‚Üí NO_REPLY

**Problem:** Ollama (3b) kann komplexe Entscheidungen NICHT treffen (zu schwach)

**L√∂sung:**
- Ollama Heartbeat = **Status Check only** ("Alles OK? Blockers?")
- Wenn Action n√∂tig ‚Üí Eskalation zu Haiku/Sonnet
- Oder: Heartbeat mit Haiku (nicht Ollama) f√ºr AS Business

**Entscheidung ben√∂tigt:** Ollama Heartbeat beibehalten (nur Status) oder wechseln zu Haiku (mit Action Logic)?

---

### 5. Model Routing f√ºr AS Business

**Frage:** Ist das Model Routing Decision Framework passend f√ºr AS-Aufgaben?

**Aktuelles Framework (4-Stufen):**
- Stufe 1: Irreversibel/Rechtlich? ‚Üí Opus (5%)
- Stufe 2-4: Complex/High-Cost/Creative? ‚Üí Haiku (85%) oder Sonnet (10%)

**AS-Aufgaben einordnen:**

| Aufgabe | Model | Rationale |
|---------|-------|-----------|
| **Lead Research (Scout)** | Haiku | Strukturiert, API Queries, kein Reasoning n√∂tig |
| **Lead Enrichment (Hektor)** | Haiku ‚Üí Sonnet | Haiku f√ºr Basic Research, Sonnet f√ºr Scoring wenn mehrdeutig |
| **Outreach Drafting (Hektor)** | Sonnet | Kreativ, Messaging, Customer-facing ‚Üí High Cost of Failure |
| **Outreach Execution** | Haiku | Strukturiert, Sending + Tracking |
| **Meeting Booking** | Haiku | Strukturiert, Calendly Integration |
| **Response Handling** | Haiku ‚Üí Sonnet | Haiku f√ºr Positive (Book Meeting), Sonnet f√ºr Questions/Objections |
| **Weekly Report** | Sonnet | Client-facing, Strategic Insights, Pr√§sentation |
| **A/B Test Analyse** | Sonnet | Analyse, Optimierung, Business Impact |
| **Client Onboarding** | Sonnet | Strategy, ICP Definition, Customer-facing |

**Verteilung:**
- Haiku: 70% (Lead Research, Enrichment Basic, Execution, Booking)
- Sonnet: 30% (Outreach Drafting, Reports, Client Onboarding, Analyse)
- Opus: 0% (keine irreversiblen Entscheidungen im AS Workflow)

**Status:** ‚úÖ Framework passt, aber Sonnet-Nutzung wird h√∂her sein als 10% (eher 30%)

**Budget-Implikation:**
- Urspr√ºnglich: ‚Ç¨105-‚Ç¨165/Mo (bei 10% Sonnet)
- AS Business: ‚Ç¨150-‚Ç¨250/Mo (bei 30% Sonnet)
- Immer noch vertretbar (99% Profit Margin)

**Action:** Budget-Erwartung adjustieren (30% Sonnet statt 10%)

---

## Zus√§tzliche Pr√ºfpunkte (von Hektor)

### 6. Memory & Logging Struktur

**Frage:** Ist Memory-Struktur passend f√ºr AS Business?

**Aktuell:**
- `memory/YYYY-MM-DD.md` ‚Äî Daily Logs
- `MEMORY.md` ‚Äî Langzeit-Memory
- `memory/trusted/`, `memory/untrusted/`, `memory/conflicts/` ‚Äî Externe Inhalte

**AS-spezifisch ben√∂tigt:**
- `memory/lead-gen/` ‚Äî Lead Database
  - `raw-leads-[client]-[date].jsonl` ‚Äî Scout's Findings
  - `enriched-leads-[client]-[date].jsonl` ‚Äî Hektor's Qualified Leads
  - `outreach-log-[client]-[date].jsonl` ‚Äî Sent Emails + Responses
  - `meetings-[client]-[date].jsonl` ‚Äî Booked Meetings + Outcomes
- `memory/clients/` ‚Äî Client Data
  - `[client-name]/` ‚Äî Folder per Client
    - `icp.json` ‚Äî Target ICP
    - `smtp.json` ‚Äî Email Config (if using client domain)
    - `metrics.json` ‚Äî Weekly Metrics
- `memory/templates/` ‚Äî Email Templates
  - `outreach-template-a.md`
  - `outreach-template-b.md`
  - `outreach-template-c.md`
  - `follow-up-day3.md`
  - `follow-up-day7.md`
  - `follow-up-day14.md`

**Action:** Erstelle Memory-Struktur (Ordner + README)

---

### 7. Error Handling & Fallbacks

**Frage:** Haben wir robuste Fehlerbehandlung f√ºr AS Workflow?

**Kritische Failure Points:**

| Failure | Impact | Fallback | Implementiert? |
|---------|--------|----------|----------------|
| **Apollo API down** | Keine neuen Leads | Fallback: LinkedIn manual scraping, Brave Search | ‚ùå FEHLT |
| **Hunter API down** | Keine Email Verification | Fallback: Basic regex check, send anyway (with disclaimer) | ‚ùå FEHLT |
| **Calendly down** | Keine Meetings bookbar | Fallback: Manual booking link in email | ‚ùå FEHLT |
| **Email sending fails** | Keine Outreach | Fallback: Retry 3x, dann alert to #alerts | ‚ùå FEHLT |
| **Inbox monitoring fails** | Keine Responses gelesen | Fallback: Manual check, alert after 24h | ‚ùå FEHLT |
| **Dashboard down** | Keine Metrics tracking | Fallback: Log to memory/lead-gen/, manual aggregation later | ‚úÖ VORHANDEN (Memory) |

**Action:** Implementiere Fallbacks f√ºr kritische APIs (Apollo, Hunter, Calendly)

---

### 8. Testing Infrastructure

**Frage:** K√∂nnen wir AS Workflow testen OHNE echte Clients/APIs?

**Ben√∂tigt:**

1. **Mock Data**
   - 50 Test-Leads (fake Namen, Emails, Companies)
   - F√ºr Testing: Lead Research ‚Üí Enrichment ‚Üí Outreach ‚Üí Booking

2. **Sandbox Modus**
   - Flag: `AS_SANDBOX_MODE=true`
   - Apollo Query ‚Üí Returns Mock Data (nicht echte API)
   - Hunter Verify ‚Üí Returns Random Scores (nicht echte API)
   - Email Sending ‚Üí Logs to file (nicht echte Emails)
   - Calendly Webhook ‚Üí Simulate Booking (nicht echte Meetings)

3. **Test Clients**
   - 3 Fake Clients (Client A, B, C)
   - Unterschiedliche ICPs (SaaS, FinTech, B2B Services)
   - Test: Can Scout/Hektor handle 3 parallel clients without cross-contamination?

**Status:** ‚ùå Sandbox Mode NICHT implementiert

**Action:** Erstelle Sandbox Mode + Mock Data (f√ºr Testing ohne echte APIs)

---

### 9. Documentation Completeness

**Frage:** Haben wir alle n√∂tigen Docs f√ºr AS Business?

**Vorhandene Docs:**
- ‚úÖ `/docs/appointment-setting-dach-scenarios.md` (21KB) ‚Äî Business Szenarien
- ‚úÖ `/docs/appointment-setting-implementation.md` (34KB) ‚Äî Implementation Plan
- ‚úÖ `/docs/lead-gen-workflow.md` (12KB) ‚Äî Original (obsolet, aber Referenz)
- ‚úÖ `/docs/lead-gen-business-scenarios.md` (13KB) ‚Äî Original (obsolet)

**Fehlende Docs:**
- [ ] **AS Playbook** ‚Äî Step-by-Step f√ºr jeden Workflow-Schritt (Scout + Hektor)
- [ ] **Email Templates Collection** ‚Äî Alle Templates mit Variationen
- [ ] **Client Onboarding Guide** ‚Äî Was Laurenz bei Sales Call fragen muss
- [ ] **Quality Standards** ‚Äî Was ist "qualified lead"? (Score-Kriterien)
- [ ] **Troubleshooting Guide** ‚Äî H√§ufige Fehler + L√∂sungen

**Action:** Erstelle fehlende Docs (AS Playbook = Priority)

---

### 10. Cost & Budget Tracking

**Frage:** K√∂nnen wir AS Business Kosten tracken?

**Kosten-Komponenten:**
- Apollo API: ‚Ç¨49/Mo (1K leads)
- Hunter API: ‚Ç¨49/Mo (1K verifications)
- Calendly: ‚Ç¨8/Mo
- OpenClaw Tokens: ~‚Ç¨50-‚Ç¨150/Mo (je nach Sonnet-Usage)
- **Total:** ‚Ç¨156-‚Ç¨256/Mo

**Revenue (konservativ, 3 Clients nach 3 Monaten):**
- 3 Clients √ó ‚Ç¨2.800/Mo = ‚Ç¨8.400/Mo

**Profit:** ‚Ç¨8.400 - ‚Ç¨256 = ‚Ç¨8.144/Mo = **97% Margin**

**Tracking ben√∂tigt:**
- Dashboard: Cost per Client (API Calls + Tokens)
- Dashboard: Revenue per Client (MRR)
- Dashboard: Profit per Client

**Status:** ‚ùå Cost Tracking NICHT im Dashboard

**Action:** Erg√§nze Cost Tracking im Dashboard (oder manuell in Google Sheets)

---

### 11. Security & DSGVO Compliance

**Frage:** Sind wir DSGVO-compliant f√ºr AS Business?

**DSGVO-Anforderungen:**
1. **Datenminimierung:** Nur speichern was n√∂tig (Name, Email, Company, Title, LinkedIn)
2. **Opt-Out:** Sofort entfernen wenn Lead "Remove me" sagt
3. **Zweckbindung:** Lead-Daten nur f√ºr AS nutzen (nicht weiterverkaufen)
4. **Transparenz:** Datenschutzerkl√§rung auf Website (wenn wir eine haben)
5. **Sicherheit:** Lead-Daten verschl√ºsselt speichern (oder zumindest passwort-gesch√ºtzt)

**Aktueller Stand:**
- ‚úÖ Datenminimierung: Wir speichern nur n√∂tiges (keine Tracking-Cookies)
- ‚úÖ Opt-Out: K√∂nnen manuell entfernen (aber: automatisiert?)
- ‚úÖ Zweckbindung: Nur AS (keine anderen Zwecke)
- ‚ùå Datenschutzerkl√§rung: Keine Website yet
- ‚ö†Ô∏è Sicherheit: Leads in `.jsonl` Files (nicht verschl√ºsselt, aber nur lokal)

**Action:** 
1. Opt-Out automatisieren (Hektor entfernt Lead sofort bei "Remove me")
2. Datenschutzerkl√§rung schreiben (f√ºr Website/Email Footer sp√§ter)
3. Lead-Daten verschl√ºsseln (optional, aber best practice)

---

### 12. Client Communication Templates

**Frage:** Haben wir alle Templates f√ºr Client Communication?

**Ben√∂tigt:**

1. **Onboarding Email** (nach Sales Call)
   ```
   Subject: Willkommen bei [Unser Service] ‚Äî N√§chste Schritte
   
   Hi [Client Name],
   
   Freut mich, dass du dabei bist!
   
   N√§chste Schritte:
   1. F√ºlle dieses Formular aus: [Link] (ICP Definition)
   2. Integriere Calendly: [Anleitung]
   3. Optional: Email Domain Setup (f√ºr bessere Deliverability)
   
   Wir starten n√§chste Woche mit deinen ersten Leads.
   
   Fragen? Schreib mir einfach.
   
   Laurenz
   ```

2. **Weekly Report Email** (jeden Freitag)
   ```
   Subject: Deine AS Report ‚Äî Woche vom [Date]
   
   Hi [Client Name],
   
   Hier deine w√∂chentliche Zusammenfassung:
   
   üìä Zahlen:
   - Leads kontaktiert: 50
   - Responses: 12 (24%)
   - Meetings gebucht: 4
   - Meetings completed: 3 (1 No-Show, ersetzt)
   - SQL Conversion: 2/3 (67%)
   
   üöÄ Top Performer:
   Template B (Social Proof) hatte beste Reply Rate (20%)
   
   üìÖ N√§chste Woche:
   - 50 neue Leads kontaktieren
   - Ziel: 5 Meetings buchen
   
   Fragen? Let me know.
   
   Best,
   Hektor
   ```

3. **Meeting Prep Email** (vor jedem Meeting)
   - Siehe Implementation Plan (bereits dokumentiert)

4. **No-Show Follow-Up** (nach verpasstem Meeting)
   ```
   Hi [Client Name],
   
   [Lead Name] hat gestern nicht teilgenommen.
   
   Ich habe bereits Ersatz gebucht: [New Lead Name] am [Date/Time].
   
   Prep Materials anbei.
   
   Hektor
   ```

**Status:** ‚ö†Ô∏è Templates existieren in Implementation Plan, aber nicht als separate Files

**Action:** Erstelle Templates als separate Files in `memory/templates/client-communication/`

---

## Zusammenfassung & N√§chste Schritte

### Kritische Blocker (MUSS vor Services)

1. ‚ùå **Dashboard AS Features** ‚Äî Option A (bauen) oder B (Workarounds)?
2. ‚ùå **AGENTS.md erg√§nzen** ‚Äî AS Workflows dokumentieren
3. ‚ùå **HEARTBEAT.md erg√§nzen** ‚Äî AS-spezifische Checks (Hektor + Scout)
4. ‚ùå **Sandbox Mode** ‚Äî Testing ohne echte APIs
5. ‚ùå **Error Handling** ‚Äî Fallbacks f√ºr Apollo/Hunter/Calendly
6. ‚ùå **Memory Struktur** ‚Äî `memory/lead-gen/`, `memory/clients/`, `memory/templates/`

### Wichtig (sollte vor Services)

7. ‚ö†Ô∏è **Ollama Heartbeat Decision** ‚Äî Beibehalten (nur Status) oder wechseln zu Haiku?
8. ‚ö†Ô∏è **AS Playbook** ‚Äî Step-by-Step Workflow-Docs
9. ‚ö†Ô∏è **Client Communication Templates** ‚Äî Separate Files
10. ‚ö†Ô∏è **DSGVO Opt-Out** ‚Äî Automatisieren

### Nice-to-Have (kann parallel)

11. üü¢ **Cost Tracking** ‚Äî Dashboard oder manuell
12. üü¢ **Dokumentation vervollst√§ndigen** ‚Äî Troubleshooting Guide, etc.

---

## Entscheidungen ben√∂tigt (von Laurenz)

1. **Dashboard Features:** Option A (bauen, 2-3 Tage) oder Option B (Workarounds)?
2. **Ollama Heartbeat:** Beibehalten (nur Status) oder wechseln zu Haiku (mit Action Logic)?
3. **Testing Strategy:** Sandbox Mode bauen (1-2 Tage) oder direkt mit echten APIs testen?
4. **Reihenfolge:** Was priorisieren? (1-6 kritische Blocker zuerst?)

---

## Gesch√§tzter Zeitaufwand

**Wenn ALLE Blocker (1-6) behoben:**
- Dashboard Features (Option A): 2-3 Tage (Claude Code)
- AGENTS.md + HEARTBEAT.md: 1-2 Stunden (Hektor)
- Sandbox Mode: 1-2 Tage (Hektor + Scout)
- Error Handling: 3-4 Stunden (Hektor)
- Memory Struktur: 30 Minuten (Hektor)

**Total:** 3-5 Tage (wenn parallel gearbeitet wird)

**Alternative (Fast Track mit Workarounds):**
- Dashboard Workarounds (Projects + Tasks): 30 Minuten
- AGENTS.md + HEARTBEAT.md: 1-2 Stunden
- Kein Sandbox (direkt mit echten APIs testen): 0 Tage
- Basic Error Handling (Retry only): 1 Stunde
- Memory Struktur: 30 Minuten

**Total Fast Track:** 3-4 Stunden (noch heute machbar)

---

## Empfehlung (Hektor)

**Fast Track Approach:**

**HEUTE (noch 2-3 Stunden):**
1. ‚úÖ AGENTS.md erg√§nzen (AS Workflows)
2. ‚úÖ HEARTBEAT.md erg√§nzen (AS Checks)
3. ‚úÖ Memory Struktur erstellen
4. ‚úÖ Basic Error Handling (Retry Logic)
5. ‚úÖ Dashboard Workarounds (Projects f√ºr Clients, Tasks f√ºr Leads)

**MORGEN (2-3 Stunden):**
6. ‚úÖ AS Playbook schreiben
7. ‚úÖ Client Communication Templates
8. ‚úÖ Test mit Mock Data (10 Fake Leads end-to-end)

**√úBERMORGEN:**
9. ‚úÖ Services abonnieren (Apollo, Hunter, Calendly)
10. ‚úÖ Erster echter Test (10 echte DACH SaaS Leads)

**Dann:** Week 1 Plan starten (wie dokumentiert)

---

**Was sagst du? Fast Track oder Full Build?**

---

## 13. OpenClaw-Potenziale aussch√∂pfen

**Frage:** Welche OpenClaw-Features nutzen wir NICHT, die f√ºr AS Business relevant w√§ren?

**Recherche:** OpenClaw Docs durchsucht (lobster, llm-task, subagents, exec-approvals, browser, plugins)

### 13.1 Lobster Workflows (SEHR RELEVANT)

**Was ist Lobster?**
- Deterministische Multi-Step Workflows mit Approval Gates
- One Tool Call statt viele (spart Tokens + LLM Orchestration)
- Resumable State (pause/resume ohne alles neu zu machen)
- Typed pipeline runtime f√ºr OpenClaw

**Wie hilft das AS Business?**

**Use Case 1: Lead Research ‚Üí Enrichment ‚Üí Outreach (End-to-End Pipeline)**

**Ohne Lobster (aktuell):**
```
Scout: Apollo query ‚Üí posts to #lead-gen ‚Üí Hektor reads
Hektor: Enrichment ‚Üí scores ‚Üí posts results
Hektor: Draft emails ‚Üí posts drafts
Laurenz: Reviews emails ‚Üí approves
Hektor: Sends emails ‚Üí tracks responses
```
‚Üí 5+ separate tool calls, LLM orchestriert jeden Schritt, keine Pause-Funktion

**Mit Lobster:**
```lobster
name: lead-pipeline
args:
  client:
    default: "Client-A"
steps:
  - id: research
    command: scout-research --client $client --limit 20 --json
  - id: enrich
    command: hektor-enrich --stdin json
    stdin: $research.stdout
  - id: draft
    command: hektor-draft-outreach --stdin json
    stdin: $enrich.stdout
  - id: approve
    command: approve --preview-from-stdin --limit 5 --prompt 'Send these emails?'
    stdin: $draft.stdout
    approval: required
  - id: send
    command: hektor-send-emails --stdin json
    stdin: $draft.stdout
    condition: $approve.approved
```

‚Üí **1 Tool Call** (`lobster run lead-pipeline.lobster --args-json '{"client":"Client-A"}'`)
‚Üí Pausiert bei Approval ‚Üí Laurenz approves ‚Üí resumes
‚Üí Spart 50-70% Tokens (LLM muss nicht orchestrieren)

**Use Case 2: Weekly Report Generation**

**Ohne Lobster:**
```
Hektor: Query dashboard metrics ‚Üí formats data ‚Üí generates insights ‚Üí posts to #lead-gen
Laurenz: Reviews report ‚Üí approves
Hektor: Sends to client
```

**Mit Lobster:**
```lobster
name: weekly-report
args:
  client:
    default: "Client-A"
steps:
  - id: metrics
    command: dashboard-metrics --client $client --week current --json
  - id: insights
    command: llm-task --prompt 'Analyze metrics and generate insights' --input-from-stdin
    stdin: $metrics.stdout
  - id: draft
    command: report-template --stdin json
    stdin: $insights.stdout
  - id: approve
    command: approve --preview-from-stdin --prompt 'Send to client?'
    stdin: $draft.stdout
    approval: required
  - id: send
    command: hektor-send-report --client $client --stdin
    stdin: $draft.stdout
    condition: $approve.approved
```

**Vorteile:**
- Deterministic (gleiche Inputs = gleiche Outputs, reproduzierbar)
- Auditierbar (alle Schritte geloggt)
- Resumable (bei Fehler oder Approval pause ‚Üí sp√§ter weiter)
- Token-effizient (LLM sieht nur Approval Request, nicht jeden Schritt)

**Status:** ‚ùå Lobster NICHT installiert, NICHT im Workflow genutzt

**Installation:**
```bash
# Install Lobster CLI
npm install -g lobster
# oder: git clone https://github.com/openclaw/lobster && cd lobster && npm install && npm link

# Enable in OpenClaw config
openclaw config set tools.alsoAllow '["lobster"]'
```

**Aufwand:** 1-2 Stunden (Installation + erste .lobster Files schreiben)

**Empfehlung:** ‚úÖ **SEHR SINNVOLL** ‚Äî Sobald AS Workflow stabil ist (nach Week 2-3), migrieren zu Lobster

---

### 13.2 LLM-Task Plugin (ERG√ÑNZEND ZU LOBSTER)

**Was ist llm-task?**
- JSON-only LLM Steps f√ºr Workflows
- Schema-validierte LLM-Outputs (keine freie Text-Antworten)
- Ideal f√ºr Lobster-Pipelines

**Wie hilft das AS Business?**

**Use Case: Lead Enrichment mit LLM**

**Aktuell (Hektor macht das manuell):**
- Brave Search nach Company ‚Üí Hektor liest Text ‚Üí extrahiert Pain Points

**Mit llm-task (in Lobster Pipeline):**
```lobster
- id: company-research
  command: brave-search --query '$company_name funding news' --json
- id: extract-pain-points
  command: llm-task --prompt 'Extract pain points from company research' --input-from-stdin --schema '{
    "type": "object",
    "properties": {
      "pain_points": { "type": "array", "items": { "type": "string" } },
      "recent_news": { "type": "string" },
      "funding_stage": { "type": "string" }
    }
  }'
  stdin: $company-research.stdout
```

**Vorteile:**
- Strukturierter Output (JSON, kein freier Text)
- Schema-validiert (sicherstellt, dass pain_points ein Array ist)
- Wiederholbar (gleiche Inputs = gleiche Struktur)

**Status:** ‚ùå llm-task Plugin NICHT enabled

**Installation:**
```json
{
  "plugins": {
    "entries": {
      "llm-task": { "enabled": true }
    }
  },
  "agents": {
    "list": [
      {
        "id": "hektor",
        "tools": { "alsoAllow": ["llm-task"] }
      }
    ]
  }
}
```

**Aufwand:** 15 Minuten (Config √§ndern + testen)

**Empfehlung:** ‚ö†Ô∏è **OPTIONAL** ‚Äî Erst wenn Lobster l√§uft, dann erg√§nzen

---

### 13.3 Sub-Agent Optimierungen (SCHON GENUTZT, ABER MEHR POTENZIAL)

**Was wir schon nutzen:**
- `sessions_spawn` f√ºr isolierte Tasks (z.B. Skill Installation)

**Was wir NICHT nutzen (aber k√∂nnten):**

**A) Sub-Agent Model Override**
```json
{
  "agents": {
    "defaults": {
      "subagents": {
        "model": "anthropic/claude-haiku-4-5",
        "thinking": "off"
      }
    }
  }
}
```

**Warum sinnvoll f√ºr AS?**
- Sub-Agent Tasks (Lead Research, Enrichment) sind oft strukturiert ‚Üí Haiku reicht
- Main Agent (Hektor) bleibt flexibel (Sonnet f√ºr Strategy)
- **Spart 50-80% Tokens** bei Sub-Agent Tasks

**B) Max Concurrent Sub-Agents**
```json
{
  "agents": {
    "defaults": {
      "subagents": {
        "maxConcurrent": 3
      }
    }
  }
}
```

**Warum sinnvoll?**
- Scout kann 3 Clients parallel researchen (Client A, B, C gleichzeitig)
- Hektor kann 3 Enrichments parallel machen
- Schnellere Execution (aber: mehr RAM/Token-Usage)

**Status:** ‚ö†Ô∏è Nutzen Sub-Agents, aber OHNE Optimierungen

**Aufwand:** 5 Minuten (Config √§ndern)

**Empfehlung:** ‚úÖ **JETZT MACHEN** ‚Äî Sub-Agent Model Override spart sofort Tokens

---

### 13.4 Browser Tool (WENIG RELEVANT, ABER ERW√ÑHNENSWERT)

**Was ist browser?**
- Eingebaute Browser-Automation (Playwright-basiert)
- Snapshots (Accessibility Tree), Screenshots, PDFs
- Element Selection via refs

**Wie k√∂nnte das AS helfen?**

**Use Case: LinkedIn Lead Research (Fallback wenn Apollo down)**
```json
{
  "action": "open",
  "targetUrl": "https://linkedin.com/search/results/people/?keywords=VP%20Sales%20SaaS%20DACH"
}
```
‚Üí Snapshot ‚Üí Parse results ‚Üí Extract Names/Titles/Companies

**Problem:**
- LinkedIn blockiert Scraping (Captcha, Rate Limits)
- Langsamer als Apollo API
- Compliance-Risiko (LinkedIn ToS)

**Status:** ‚ùå Browser Tool NICHT f√ºr AS genutzt

**Empfehlung:** ‚ùå **NICHT PRIORISIEREN** ‚Äî Apollo + Hunter reichen, Browser nur als letzter Fallback

---

### 13.5 Custom Plugins (ADVANCED, SP√ÑTER)

**Was ist plugin system?**
- Eigene Tools als Plugins bauen (Node.js/TypeScript)
- Registrieren in OpenClaw (`tools.plugin`)

**Wie k√∂nnte das helfen?**

**Use Case: Apollo/Hunter als Custom Plugin**
- Wrapper um Apollo/Hunter APIs (statt Shell-Scripts)
- Typsicher, bessere Error Handling
- Caching (gleiche Query mehrfach ‚Üí cached Result)

**Beispiel:**
```typescript
// plugins/apollo-plugin.ts
export async function searchLeads(query: string, limit: number): Promise<Lead[]> {
  const response = await fetch(`https://api.apollo.io/v1/people/search`, {
    headers: { Authorization: `Bearer ${process.env.APOLLO_API_KEY}` },
    body: JSON.stringify({ q: query, per_page: limit })
  });
  return response.json();
}
```

**Status:** ‚ùå Keine Custom Plugins gebaut

**Aufwand:** 1-2 Tage (Plugin Architektur lernen + bauen)

**Empfehlung:** ‚ùå **NICHT JETZT** ‚Äî Erst wenn AS Business l√§uft (Month 3+)

---

### 13.6 Exec Approvals (SICHERHEIT, WENIGER AS-RELEVANT)

**Was ist exec approvals?**
- Allowlists f√ºr `exec` Commands (Sandbox-Escape Schutz)
- User muss explizit erlauben (√§hnlich wie Lobster approvals)

**F√ºr AS Business relevant?**
- Nein, wir f√ºhren keine kritischen System-Commands aus
- Scout/Hektor nutzen APIs (Apollo, Hunter), nicht Shell-Commands

**Empfehlung:** ‚ùå **NICHT RELEVANT** f√ºr AS Business

---

## Zusammenfassung: OpenClaw-Potenziale

| Feature | Relevanz f√ºr AS | Aufwand | Empfehlung | Wann? |
|---------|----------------|---------|------------|-------|
| **Lobster Workflows** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 1-2 Std | **SEHR SINNVOLL** | Week 2-3 (nach Workflow stabilisiert) |
| **LLM-Task Plugin** | ‚≠ê‚≠ê‚≠ê | 15 Min | OPTIONAL | Nach Lobster |
| **Sub-Agent Model Override** | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 Min | **JETZT** | Sofort (spart Tokens) |
| **Sub-Agent Concurrency** | ‚≠ê‚≠ê | 5 Min | OPTIONAL | Bei 3+ Clients parallel |
| **Browser Tool** | ‚≠ê | N/A | NICHT | Nur Fallback |
| **Custom Plugins** | ‚≠ê‚≠ê | 1-2 Tage | NICHT | Month 3+ |
| **Exec Approvals** | ‚≠ê | N/A | NICHT | Nicht relevant |

---

## Konkrete Actions (OpenClaw-Potenziale)

**JETZT (sofort, 5 Min):**
1. ‚úÖ Sub-Agent Model Override aktivieren (Haiku f√ºr Sub-Agents)
   ```bash
   openclaw config set agents.defaults.subagents.model "anthropic/claude-haiku-4-5"
   openclaw config set agents.defaults.subagents.thinking "off"
   openclaw gateway restart
   ```

**WEEK 2-3 (nach AS Workflow l√§uft):**
2. ‚ö†Ô∏è Lobster installieren + erste .lobster Files schreiben
   - `lead-pipeline.lobster` (Research ‚Üí Enrichment ‚Üí Outreach ‚Üí Approval ‚Üí Send)
   - `weekly-report.lobster` (Metrics ‚Üí Insights ‚Üí Draft ‚Üí Approval ‚Üí Send)
   - Test: 1 Client end-to-end mit Lobster

3. ‚ö†Ô∏è llm-task Plugin aktivieren (f√ºr strukturierte Enrichment-Steps)

**SP√ÑTER (Month 3+):**
4. üü¢ Custom Plugins (Apollo/Hunter Wrapper) wenn n√∂tig
5. üü¢ Sub-Agent Concurrency erh√∂hen (wenn 5+ Clients parallel)

---

## Budget-Implikation (Sub-Agent Model Override)

**Aktuell (kein Override):**
- Sub-Agent nutzt Main Model (Sonnet)
- Beispiel: Skill Installation Task = 4K Tokens √ó Sonnet = ‚Ç¨0.06

**Mit Override (Haiku):**
- Sub-Agent nutzt Haiku
- Gleiche Task = 4K Tokens √ó Haiku = ‚Ç¨0.002
- **Ersparnis: 97%** (‚Ç¨0.058/Task)

**Bei 20 Sub-Agent Tasks/Monat:**
- Vorher: ‚Ç¨1.20/Mo
- Nachher: ‚Ç¨0.04/Mo
- **Ersparnis: ‚Ç¨1.16/Mo** (klein, aber adds up)

---

**Was sagst du? Fast Track oder Full Build?**
